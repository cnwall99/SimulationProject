---
title: "Simulation 2"
author: "cnwall2"
date: "2024-10-16"
output: html_document
editor_options: 
  chunk_output_type: console
---
# Study 2: Selecting the Appropriate Model Via RMSE

### <u>**Introduction**</u>
This study was performed to determine whether comparing RMSE is a viable and repeatable method for determining which model is appropriate for a given dataset. To do this, data was simulated from the following model:</br></br>
<center>**$Y_i = \beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + \beta_3 x_{i3} + \beta_4 x_{i4} + \beta_5 x_{i5} + \beta_6 x_{i6} + \epsilon_i$**</center></br></br>
where:</br>

<ul>
  <li> $\epsilon_i \sim N(0, \sigma^2)$
  <li> $\beta_0 = 0$</li>
  <li> $\beta_1 = 3$</li>
  <li> $\beta_2 = -4$</li>
  <li> $\beta_3 = 1.6$</li>
  <li> $\beta_4 = -1.1$</li>
  <li> $\beta_5 = 0.7$</li>
  <li> $\beta_6 = 0.5$</li>
</ul>

Using the above model, the following MLR models will be formed:</br></br>
<ul>
  <li>$y \sim x1$</li> 
  <li>$y \sim x1 + x2$</li> 
  <li>$y \sim x1 + x2 + x3$</li> 
  <li>$y \sim x1 + x2 + x3 + x4$</li> 
  <li>$y \sim x1 + x2 + x3 + x4 + x5$</li> 
  <li>$y \sim x1 + x2 + x3 + x4 + x5 + x6$ (**The correct model**)</li> 
  <li>$y \sim x1 + x2 + x3 + x4 + x5 + x6 + x7$</li> 
  <li>$y \sim x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8$</li> 
  <li>$y \sim x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8 + x9$</li>
</ul>

Each model will be simulated with a constant sample size of $n = 500$, and for each of three values of the standard deviation ($\sigma \in (1, 2, 4$)). The RMSE will be calculated for each simulation and model according to the equation below:</br></br>
<center>**RMSE(actual, fitted)** = $\sqrt{\frac{1}{n}\sum_{i = 1}^{n}(y_i - \hat{y}_i)^2}$</center>

Prior to simulating, the generated data will be split into a training and test set. For each simulation, the RMSE will be calculated for both the training and test set. While the training set RMSE is calculated for informational purposes, the test RMSE is used to determine the "selected" model. The model with the lowest test RMSE will be recorded for each simulation, which will allow for the determination of the effectiveness of this algorithm.

### <u>**Methods**</u>
First, a random seed was set to enusre repeatability.
```{r setup}
knitr::opts_chunk$set(warning = FALSE, message = FALSE)
knitr::opts_chunk$set(fig.width=10, fig.height=6) 
#Setting random seed
birthday = 19990619
set.seed(birthday)
```

Constants were then initialized, and a dataframe was constructed for the dataset.
```{r}
#Model - Y = B0 + B1(x1) + B2(x2) + ..... + B6(x6) + e

b0 = 0
b1 = 3
b2 = -4
b3 = 1.6
b4 = -1.1
b5 = 0.7
b6 = 0.5
b7 = 0
b8 = 0
b9 = 0
n = 500
sigma = c(1, 2, 4)
nsims = 10000
study2 = read.csv("study_2.csv")
ntrain = 250



#initialize two 3 X n lists of vectors, train_rsme and test_rsme
train_rsme = list(list(
  rep(0, nsims),
  rep(0, nsims),
  rep(0, nsims),
  rep(0, nsims),
  rep(0, nsims),
  rep(0, nsims),
  rep(0, nsims),
  rep(0, nsims),
  rep(0, nsims)
),
list(
  rep(0, nsims),
  rep(0, nsims),
  rep(0, nsims),
  rep(0, nsims),
  rep(0, nsims),
  rep(0, nsims),
  rep(0, nsims),
  rep(0, nsims),
  rep(0, nsims)
),
list(
  rep(0, nsims),
  rep(0, nsims),
  rep(0, nsims),
  rep(0, nsims),
  rep(0, nsims),
  rep(0, nsims),
  rep(0, nsims),
  rep(0, nsims),
  rep(0, nsims)
))


test_rsme = list(list(
  rep(0, nsims),
  rep(0, nsims),
  rep(0, nsims),
  rep(0, nsims),
  rep(0, nsims),
  rep(0, nsims),
  rep(0, nsims),
  rep(0, nsims),
  rep(0, nsims)
),
list(
  rep(0, nsims),
  rep(0, nsims),
  rep(0, nsims),
  rep(0, nsims),
  rep(0, nsims),
  rep(0, nsims),
  rep(0, nsims),
  rep(0, nsims),
  rep(0, nsims)
),
list(
  rep(0, nsims),
  rep(0, nsims),
  rep(0, nsims),
  rep(0, nsims),
  rep(0, nsims),
  rep(0, nsims),
  rep(0, nsims),
  rep(0, nsims),
  rep(0, nsims)
))

#initialize a list of vectors which holds what model is selected by the algorithm
model_sel = list(
  rep(0, nsims),
  rep(0, nsims),
  rep(0, nsims)
)
```

Each of the nine models was then simulated 1000 times for each level of standard deviation. The models were then cross-validated on the test set, and the RMSE was calculated. The model with the smallest test RMSE is then recorded.
```{r}
#Create a function to calculate rmse
rmse = function(actual, fitted, n) {
  sqrt((1 / n) * sum((actual - fitted) ^ 2))
}


#Iterate through each value of sigma
for(i in 1:length(sigma)){
  #Simulate nsims times
  for(j in 1:nsims){
    #select a random sample for trainnig and testing
    train_data_idx = sample(1:nrow(study2), ntrain)
    
    #calculate noise
    eps = rnorm(ntrain, mean = 0, sd = sigma[i])
    #calculate y
    study2$y = b0 + b1*study2$x1 + b2*study2$x2 + b3*study2$x3 + b4*study2$x4 + b5*study2$x5 + b6*study2$x6 + b7*study2$x7 + b8*study2$x8 + b9*study2$x9
    
    train = study2[train_data_idx, ]
    test = study2[-train_data_idx, ]
    
    mod1 = lm(
      y ~ x1,
      data = train
    )
    mod2 = lm(
      y ~ x1 + x2,
      data = train
    )
    mod3 = lm(
      y ~ x1 + x2 + x3,
      data = train
    )
    mod4 = lm(
      y ~ x1 + x2 + x3 + x4,
      data = train
    )
    mod5 = lm(
      y ~ x1 + x2 + x3 + x4 + x5,
      data = train
    )
    mod6 = lm(
      y ~ x1 + x2 + x3 + x4 + x5 + x6,
      data = train
    )
    mod7 = lm(
      y ~ x1 + x2 + x3 + x4 + x5 + x6 + x7,
      data = train
    )
    mod8 = lm(
      y ~ x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8,
      data = train
    )
    mod9 = lm(
      y ~ x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8 + x9,
      data = train
    )
    
    train_error = c(
      rmse(train$y, predict(mod1, train), ntrain),
      rmse(train$y, predict(mod2, train), ntrain),
      rmse(train$y, predict(mod3, train), ntrain),
      rmse(train$y, predict(mod4, train), ntrain),
      rmse(train$y, predict(mod5, train), ntrain),
      rmse(train$y, predict(mod6, train), ntrain),
      rmse(train$y, predict(mod7, train), ntrain),
      rmse(train$y, predict(mod8, train), ntrain),
      rmse(train$y, predict(mod9, train), ntrain)
    )
    
    test_error = c(
      rmse(test$y, predict(mod1, test), ntrain),
      rmse(test$y, predict(mod2, test), ntrain),
      rmse(test$y, predict(mod3, test), ntrain),
      rmse(test$y, predict(mod4, test), ntrain),
      rmse(test$y, predict(mod5, test), ntrain),
      rmse(test$y, predict(mod6, test), ntrain),
      rmse(test$y, predict(mod7, test), ntrain),
      rmse(test$y, predict(mod8, test), ntrain),
      rmse(test$y, predict(mod9, test), ntrain)
    )
    
    model_sel[[i]][j] = which.min(test_error)
    train_rsme[[i]][[1]][j] = train_error[1]
    train_rsme[[i]][[2]][j] = train_error[2]
    train_rsme[[i]][[3]][j] = train_error[3]
    train_rsme[[i]][[4]][j] = train_error[4]
    train_rsme[[i]][[5]][j] = train_error[5]
    train_rsme[[i]][[6]][j] = train_error[6]
    train_rsme[[i]][[7]][j] = train_error[7]
    train_rsme[[i]][[8]][j] = train_error[8]
    train_rsme[[i]][[9]][j] = train_error[9]
    
    test_rsme[[i]][[1]][j] = test_error[1]
    test_rsme[[i]][[2]][j] = test_error[2]
    test_rsme[[i]][[3]][j] = test_error[3]
    test_rsme[[i]][[4]][j] = test_error[4]
    test_rsme[[i]][[5]][j] = test_error[5]
    test_rsme[[i]][[6]][j] = test_error[6]
    test_rsme[[i]][[7]][j] = test_error[7]
    test_rsme[[i]][[8]][j] = test_error[8]
    test_rsme[[i]][[9]][j] = test_error[9]
    
  }
}
```

The RMSE values were then averaged according to the model and value of sigma.
```{r}
#averaging train and test rsme for each value of sigma and model
avg_train_rsme = list(
  c(
    mean(train_rsme[[1]][[1]]),
    mean(train_rsme[[1]][[2]]),
    mean(train_rsme[[1]][[3]]),
    mean(train_rsme[[1]][[4]]),
    mean(train_rsme[[1]][[5]]),
    mean(train_rsme[[1]][[6]]),
    mean(train_rsme[[1]][[7]]),
    mean(train_rsme[[1]][[8]]),
    mean(train_rsme[[1]][[9]])
  ),
  c(
    mean(train_rsme[[2]][[1]]),
    mean(train_rsme[[2]][[2]]),
    mean(train_rsme[[2]][[3]]),
    mean(train_rsme[[2]][[4]]),
    mean(train_rsme[[2]][[5]]),
    mean(train_rsme[[2]][[6]]),
    mean(train_rsme[[2]][[7]]),
    mean(train_rsme[[2]][[8]]),
    mean(train_rsme[[2]][[9]])
  ),
  c(
    mean(train_rsme[[3]][[1]]),
    mean(train_rsme[[3]][[2]]),
    mean(train_rsme[[3]][[3]]),
    mean(train_rsme[[3]][[4]]),
    mean(train_rsme[[3]][[5]]),
    mean(train_rsme[[3]][[6]]),
    mean(train_rsme[[3]][[7]]),
    mean(train_rsme[[3]][[8]]),
    mean(train_rsme[[3]][[9]])
  )
)

avg_test_rsme = list(
  c(
    mean(test_rsme[[1]][[1]]),
    mean(test_rsme[[1]][[2]]),
    mean(test_rsme[[1]][[3]]),
    mean(test_rsme[[1]][[4]]),
    mean(test_rsme[[1]][[5]]),
    mean(test_rsme[[1]][[6]]),
    mean(test_rsme[[1]][[7]]),
    mean(test_rsme[[1]][[8]]),
    mean(test_rsme[[1]][[9]])
  ),
  c(
    mean(test_rsme[[2]][[1]]),
    mean(test_rsme[[2]][[2]]),
    mean(test_rsme[[2]][[3]]),
    mean(test_rsme[[2]][[4]]),
    mean(test_rsme[[2]][[5]]),
    mean(test_rsme[[2]][[6]]),
    mean(test_rsme[[2]][[7]]),
    mean(test_rsme[[2]][[8]]),
    mean(test_rsme[[2]][[9]])
  ),
  c(
    mean(test_rsme[[3]][[1]]),
    mean(test_rsme[[3]][[2]]),
    mean(test_rsme[[3]][[3]]),
    mean(test_rsme[[3]][[4]]),
    mean(test_rsme[[3]][[5]]),
    mean(test_rsme[[3]][[6]]),
    mean(test_rsme[[3]][[7]]),
    mean(test_rsme[[3]][[8]]),
    mean(test_rsme[[3]][[9]])
  )
)

correct1 = sum(model_sel[[1]] == 6) / length(model_sel[[1]])
correct2 = sum(model_sel[[2]] == 6) / length(model_sel[[2]])
correct3 = sum(model_sel[[3]] == 6) / length(model_sel[[3]])
```


### <u>**Results**</u>
```{r}
#plot average test_rsme vs model size
par(mfrow = c(2,3))
plot(seq(1:9), avg_train_rsme[[1]], col = "blue", pch = 19, xlab = "Number of Factors", ylab = "Average Train RSME", main = paste("Train RSME vs Factors: Sigma = ", sigma[1]), cex = 2)
axis(1, at = seq(1:9))
plot(seq(1:9), avg_train_rsme[[2]], col = "blue", pch = 19, xlab = "Number of Factors", ylab = "Average Train RSME", main = paste("Train RSME vs Factors: Sigma = ", sigma[2]), cex = 2)
axis(1, at = seq(1:9))
plot(seq(1:9), avg_train_rsme[[3]], col = "blue", pch = 19, xlab = "Number of Factors", ylab = "Average Train RSME", main = paste("Train RSME vs Factors: Sigma = ", sigma[3]), cex = 2)
axis(1, at = seq(1:9))
plot(seq(1:9), avg_test_rsme[[1]], col = "blue", pch = 19, xlab = "Number of Factors", ylab = "Average Test RSME", main = paste("Test RSME vs Factors: Sigma = ", sigma[1]), cex = 2)
axis(1, at = seq(1:9))
plot(seq(1:9), avg_test_rsme[[2]], col = "blue", pch = 19, xlab = "Number of Factors", ylab = "Average Test RSME", main = paste("Test RSME vs Factors: Sigma = ", sigma[2]), cex = 2)
axis(1, at = seq(1:9))
plot(seq(1:9), avg_test_rsme[[3]], col = "blue", pch = 19, xlab = "Number of Factors", ylab = "Average Test RSME", main = paste("Test RSME vs Factors: Sigma = ", sigma[3]), cex = 2)
axis(1, at = seq(1:9))

```

```{r}
par(mfrow = c(1,3))
hist1 = hist(model_sel[[1]], breaks = seq(1:9), xlab = "Model #", ylab = "Times Selected", main = paste("Model Selections by Test RMSE - Sigma = ", sigma[1]))
axis(1, at = seq(1:9))
hist(model_sel[[2]], breaks = seq(1:9), xlab = "Model #", ylab = "Times Selected", main = paste("Model Selections by Test RMSE - Sigma = ", sigma[2]))
axis(1, at = seq(1:9))
hist(model_sel[[3]], breaks = seq(1:9), xlab = "Model #", ylab = "Times Selected", main = paste("Model Selections by Test RMSE - Sigma = ", sigma[3]))
axis(1, at = seq(1:9))
```

### <u>**Discussion**</u>
As is shown above, the average test RMSE for each level of sigma is the smallest when the correct model (significant up to $\beta_6$) is used. The following table shows the proportion of simulations in which the correct model was chosen for each level of sigma.

|              Sigma              | Proportion Correct |
|:-------------------------------:|:------------------:|
| `r paste("Sigma = ", sigma[1])` |    `r correct1`    |
| `r paste("Sigma = ", sigma[2])` |    `r correct2`    |
| `r paste("Sigma = ", sigma[3])` |    `r correct3`    |


Due to these results, it seems that comparing cross-validation RMSE values on test data is a relatively reliable and repeatable method for determining an appropriate model. In the worst case of this study (largest standard deviation), this algorithm selected the wrong model `r (1- correct3) * 100`% of the time.</br></br>
Based on this study, the proportion of incorrectly selected models increases as noise (standard deviation increases).



